{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pdzoklbO6D9e"
   },
   "source": [
    "## Speaker Verification using the Siamese Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KrbLmOhQYiBI"
   },
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from google.colab import drive\n",
    "import pickle\n",
    "import librosa\n",
    "from itertools import combinations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5-nZHcjqqp4e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DzhYQTmARQq6"
   },
   "outputs": [],
   "source": [
    "\n",
    "f = open('/content/gdrive/My Drive/Deep Learning HW 4/hw4_trs.pkl','rb') \n",
    "train_file = pickle.load(f)\n",
    "\n",
    "\n",
    "f = open('/content/gdrive/My Drive/Deep Learning HW 4/hw4_tes.pkl','rb') \n",
    "test_file = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuAANJAdRqQS"
   },
   "outputs": [],
   "source": [
    "#Converting the files to stft and getting the absolute value\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "\n",
    "for s in train_file:\n",
    "  train.append(np.abs(librosa.stft(s, n_fft=1024, hop_length=512)).T)\n",
    "\n",
    "for s in test_file:\n",
    "  test.append(np.abs(librosa.stft(s, n_fft=1024, hop_length=512)).T)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbW7FWQ6Sx6Y"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Sampling and creating the dataset for train and test\n",
    "\n",
    "def generate_dataset(data):\n",
    "  speaker_len = len(data)//10\n",
    "  pairs_1 = []\n",
    "  pairs_2 = []\n",
    "  y_bool  = []  \n",
    "  for i in range(1,speaker_len+1):\n",
    "    pos_set = data[i*10 - 10:i*10]\n",
    "    neg_set = np.delete(data,range(i*10 - 10,i*10),axis=0)\n",
    "    idx = list(combinations(range(10), 2))\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for i in idx:\n",
    "      l1.append(pos_set[i[0]])\n",
    "    for j in idx:\n",
    "      l2.append(pos_set[j[1]])\n",
    "    pairs_1 = pairs_1 +  l1\n",
    "    pairs_2 = pairs_2 + l2\n",
    "    y_bool  = y_bool+[1]*len(idx)\n",
    "    idx = list(zip(np.random.randint(10,size=45),np.random.randint(len(data)-10,size=45)))\n",
    "    l3 = []\n",
    "    l4 = []\n",
    "    for i1 in idx:\n",
    "      l3.append(pos_set[i1[0]])\n",
    "    for j1 in idx:\n",
    "      l4.append(neg_set[j1[1]])\n",
    "    pairs_1 = pairs_1 + l3\n",
    "    pairs_2 = pairs_2 + l4\n",
    "    y_bool  = y_bool + [0]*len(idx)\n",
    "  s = np.arange(np.array(pairs_1).shape[0])\n",
    "  np.random.shuffle(s)\n",
    "  return np.array(pairs_1)[s],np.array(pairs_2)[s],np.array(y_bool)[s]\n",
    "\n",
    "#Train and test data for network\n",
    "left_train,right_train,y_train = generate_dataset(train)\n",
    "left_test,right_test,y_test = generate_dataset(test)\n",
    "\n",
    "left_train_len = []\n",
    "left_test_len = []\n",
    "\n",
    "for i in left_train:\n",
    "  left_train_len.append(i.shape[0])\n",
    "\n",
    "for j in left_test:\n",
    "  left_test_len.append(j.shape[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7JqwnVw23of"
   },
   "source": [
    "A GRU cell stacked model is used.\n",
    "Batch Normalization is implemented\n",
    "The layer outputs are being normalized for cosine similarity.\n",
    "Dot product of the outputs is performed and are wrapper in a sigmoid function. Cross entropy loss function combined with Adam optimizer is implemented.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rSWlD-hNWFe2"
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_batch(x1,x2,y,seq_len,start,batch_size):\n",
    "     return x1[start:start + batch_size],x2[start:start + batch_size], y[start:start + batch_size],seq_len[start:start + batch_size]\n",
    "                                  \n",
    "\n",
    "n_classes = 512\n",
    "n_channels = 1\n",
    "hidden_units = 512\n",
    "\n",
    "\n",
    "batch_size = 90\n",
    "n_epoch = 100\n",
    "display_f = 10\n",
    "keep_prob = 0.9\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "def gru_cell():\n",
    "    gru_cell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.GRUCell(hidden_units, \n",
    "                                                          kernel_initializer =tf.contrib.layers.variance_scaling_initializer()),\n",
    "                                                          output_keep_prob = 1 - dropout)\n",
    "    return gru_cell\n",
    "\n",
    "def batch_norm_layer(x,train_phase,momentum=0.9,epsilon=0.001):\n",
    "  return tf.layers.batch_normalization(\n",
    "        inputs=x,\n",
    "        axis=-1,\n",
    "        momentum=momentum,\n",
    "        epsilon=epsilon,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        reuse= False,\n",
    "        training = train_phase)\n",
    "\n",
    "  \n",
    "\n",
    "tf.reset_default_graph()\n",
    "left = tf.placeholder(tf.float32,[None,None,513])\n",
    "right = tf.placeholder(tf.float32,[None,None,513])\n",
    "y = tf.placeholder(tf.float32,[None,1])\n",
    "succ_length = tf.placeholder(tf.int32,None)\n",
    "flag_training=tf.placeholder(tf.bool)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def siamese_model(x,flag_training):\n",
    "    with tf.name_scope(\"model\"):\n",
    "\n",
    "        with tf.variable_scope(\"rnn\",reuse = tf.AUTO_REUSE ) as scope:\n",
    "              rnn_cell = tf.contrib.rnn.MultiRNNCell([gru_cell() for cell in range(2)])\n",
    "              dynamic_rnn, _  = tf.nn.dynamic_rnn(rnn_cell, x, dtype=tf.float32,sequence_length=succ_length)\n",
    "\n",
    "\n",
    "        with tf.variable_scope(\"dense\",reuse = tf.AUTO_REUSE ) as scope: \n",
    "              batchnorm_layer = batch_norm_layer(dynamic_rnn,flag_training)\n",
    "              dense_layer = tf.layers.dense(batchnorm_layer, n_classes, kernel_initializer= tf.contrib.layers.variance_scaling_initializer(),activation=tf.nn.tanh)\n",
    "        output = tf.layers.flatten(dense_layer)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "left_output =  tf.nn.l2_normalize(siamese_model(left,flag_training),0)\n",
    "right_output =  tf.nn.l2_normalize(siamese_model(right,flag_training),0)\n",
    "\n",
    "\n",
    "layer_dot = tf.reduce_sum(tf.multiply( left_output, right_output ),1, keep_dims=True)\n",
    "\n",
    "sig_layer = tf.sigmoid(layer_dot)\n",
    "\n",
    "\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=sig_layer))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.round(sig_layer),y),tf.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "2WOaMhs5cHln",
    "outputId": "cc5bf6f9-1575-4f79-d999-44dc7fb449c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0  Test Loss = 0.81112534  Test Accuracy = 0.5\n",
      "Epoch 1  Test Loss = 0.80861586  Test Accuracy = 0.5038889\n",
      "Epoch 2  Test Loss = 0.771661  Test Accuracy = 0.54555553\n",
      "Epoch 3  Test Loss = 0.725836  Test Accuracy = 0.5966667\n",
      "Epoch 4  Test Loss = 0.71141577  Test Accuracy = 0.62333333\n",
      "Epoch 5  Test Loss = 0.71551013  Test Accuracy = 0.6077778\n",
      "Epoch 6  Test Loss = 0.6962006  Test Accuracy = 0.6477778\n",
      "Epoch 7  Test Loss = 0.7592695  Test Accuracy = 0.5422222\n",
      "Epoch 8  Test Loss = 0.7186783  Test Accuracy = 0.62277776\n",
      "Epoch 9  Test Loss = 0.6969394  Test Accuracy = 0.6483333\n",
      "Epoch 10  Test Loss = 0.6936519  Test Accuracy = 0.6627778\n",
      "Epoch 11  Test Loss = 0.68449235  Test Accuracy = 0.68333334\n",
      "Epoch 12  Test Loss = 0.68582046  Test Accuracy = 0.67777777\n",
      "Epoch 13  Test Loss = 0.6832045  Test Accuracy = 0.68333334\n",
      "Epoch 14  Test Loss = 0.6813983  Test Accuracy = 0.69722223\n",
      "Epoch 15  Test Loss = 0.6805864  Test Accuracy = 0.69\n",
      "Epoch 16  Test Loss = 0.6780084  Test Accuracy = 0.69777775\n",
      "Epoch 17  Test Loss = 0.7149123  Test Accuracy = 0.62\n",
      "Epoch 18  Test Loss = 0.70915926  Test Accuracy = 0.62166667\n",
      "Epoch 19  Test Loss = 0.6913909  Test Accuracy = 0.66888887\n",
      "Epoch 20  Test Loss = 0.66972333  Test Accuracy = 0.7133333\n",
      "CPU times: user 8min 15s, sys: 1min 19s, total: 9min 34s\n",
      "Wall time: 14min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Running the session\n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init_global)\n",
    "sess.run(init_local)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "  for i in range(len(left_train)//batch_size):\n",
    "      \n",
    "      \n",
    "    epoch_x1,epoch_x2,epoch_y,seq_len_batch = batch_next(left_train,right_train,y_train,left_train_len,i*batch_size,batch_size)\n",
    "    epoch_y = epoch_y.reshape([batch_size,1])\n",
    "\n",
    "                             \n",
    "    _,c = sess.run([optimizer,loss], feed_dict={left:epoch_x1,right:epoch_x2,y:epoch_y,succ_length:seq_len_batch,flag_training:True})\n",
    "\n",
    "    \n",
    "    \n",
    "    epoch_loss,acc = sess.run([loss,accuracy],\\\n",
    "                              feed_dict={left:left_test,\n",
    "                                         right:right_test,\n",
    "                                         y:y_test.reshape([len(y_test),1]),succ_length:left_test_len,flag_training:False})\n",
    "    \n",
    "    \n",
    "    \n",
    "         \n",
    "  print(\"Epoch\", epoch, \" Test Loss =\", epoch_loss, \" Test Accuracy =\", acc)\n",
    "  if(acc>=0.70):\n",
    "      break;\n",
    "  \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlWAJ7FcfSjs"
   },
   "outputs": [],
   "source": [
    "acc = sess.run(accuracy,feed_dict={left:left_test,right:right_test,y:y_test.reshape([len(y_test),1]),succ_length:left_test_len,flag_training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TiJLOLpefZWs",
    "outputId": "d4f8d13e-1e03-40b4-d868-ec81217c1483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy is  71.38888835906982\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy is \",acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4iRUsv0yruKG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Deep_Learning_HW4_Prahasan.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
